{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96fe29c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import kagglehub\n",
    "import networkx as nx\n",
    "from networkx.readwrite import json_graph\n",
    "from kagglehub import KaggleDatasetAdapter\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# IF YOU NEED TO DOWNLOAD THE ARXIV DATASET, UNCOMMENT BELOW:\n",
    "# file_path = kagglehub.dataset_download(\"Cornell-University/arxiv\")\n",
    "# print(\"Downloaded to:\", file_path)\n",
    "# json_path = os.path.join(file_path, \"arxiv-metadata-oai-snapshot.json\")\n",
    "# print(\"JSON snapshot path:\", json_path)\n",
    "\n",
    "# subset_df = pd.read_json(json_path, lines=True, nrows=10_000)\n",
    "# print(\"Subset shape:\", subset_df.shape)\n",
    "# print(subset_df[[\"id\", \"title\", \"categories\"]].head(3))\n",
    "# os.makedirs(\"data\", exist_ok=True)\n",
    "# subset_path = \"data/arxiv_subset_10k.jsonl\"\n",
    "# subset_df.to_json(subset_path, orient=\"records\", lines=True)\n",
    "# print(f\"Saved trimmed subset → {subset_path}\")\n",
    "\n",
    "# os.remove(json_path)\n",
    "# print(\"Deleted large original file.\")\n",
    "\n",
    "df = pd.read_json(\"data/arxiv_subset_10k.jsonl\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258cc1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "print(\"SBERT model loaded for embedding abstracts.\\n\\n\")\n",
    "print(df[\"categories\"].value_counts().head(10), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45adabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Column containing the author data\n",
    "AUTHOR_COL = 'authors_parsed'\n",
    "\n",
    "# --- 1. Parse the authors_parsed column safely ---\n",
    "def parse_authors_field(val):\n",
    "    \"\"\"Convert a possibly stringified authors_parsed field to a Python list.\"\"\"\n",
    "    if isinstance(val, list):\n",
    "        return val\n",
    "    if isinstance(val, str):\n",
    "        for parser in (ast.literal_eval, json.loads):\n",
    "            try:\n",
    "                parsed = parser(val)\n",
    "                if isinstance(parsed, list):\n",
    "                    return parsed\n",
    "            except Exception:\n",
    "                continue\n",
    "        return []\n",
    "    return []\n",
    "\n",
    "df[AUTHOR_COL] = df[AUTHOR_COL].apply(parse_authors_field)\n",
    "\n",
    "# --- 2. Compute number of authors per row for sanity checking ---\n",
    "df['n_authors'] = df[AUTHOR_COL].apply(lambda x: len(x) if isinstance(x, list) else 0)\n",
    "print(\"Summary of author counts per article:\\n\", df['n_authors'].describe())\n",
    "\n",
    "# --- 3. Define cleaning helper ---\n",
    "def handle_author(author_parsed_instance):\n",
    "    \"\"\"Convert ['Ortega-Cerda', 'Joaquim', ''] -> 'Joaquim|Ortega-Cerda'\"\"\"\n",
    "    try:\n",
    "        first = author_parsed_instance[1].strip() if len(author_parsed_instance) > 1 else ''\n",
    "        last = author_parsed_instance[0].strip() if len(author_parsed_instance) > 0 else ''\n",
    "        return f\"{first}|{last}\"\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "# --- 4. Iterate efficiently and collect unique authors ---\n",
    "unique_authors = set()\n",
    "\n",
    "for row in tqdm(df.itertuples(index=False), total=len(df)):\n",
    "    authors = getattr(row, AUTHOR_COL, [])\n",
    "    if not isinstance(authors, list):\n",
    "        continue\n",
    "    for a in authors:\n",
    "        author_clean = handle_author(a)\n",
    "        if author_clean and \"|\" in author_clean:\n",
    "            unique_authors.add(author_clean)\n",
    "\n",
    "# --- 5. Print diagnostics ---\n",
    "df = df[df['n_authors'] < 10000].copy()\n",
    "# print(f\"Dropped {len(df) - len(df)} corrupted rows.\")\n",
    "\n",
    "print(f\"\\n✅ Total rows processed: {len(df)}\")\n",
    "print(f\"✅ Total unique authors: {len(unique_authors)}\")\n",
    "print(\"Example authors:\", list(unique_authors)[:10])\n",
    "\n",
    "# Optionally: sanity check extreme rows\n",
    "print(\"\\nTop 5 articles with the most authors:\")\n",
    "print(df.nlargest(5, 'n_authors')[[AUTHOR_COL, 'n_authors']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950aae07",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = np.zeros((len(unique_authors), 384))\n",
    "from collections import defaultdict\n",
    "\n",
    "# Prepare data structures\n",
    "author_sums = defaultdict(lambda: np.zeros(384))\n",
    "author_counts = defaultdict(int)\n",
    "\n",
    "# Iterate once over all rows\n",
    "for row in tqdm(df.itertuples(index=False), total=len(df)):\n",
    "    authors = getattr(row, 'authors_parsed', [])\n",
    "    if not isinstance(authors, list) or len(authors) == 0:\n",
    "        continue\n",
    "\n",
    "    # Combine title + abstract text\n",
    "    text_parts = []\n",
    "    if hasattr(row, 'title') and isinstance(row.title, str):\n",
    "        text_parts.append(row.title)\n",
    "    if hasattr(row, 'abstract') and isinstance(row.abstract, str):\n",
    "        text_parts.append(row.abstract)\n",
    "    if not text_parts:\n",
    "        continue\n",
    "    combined_text = \" \".join(text_parts)\n",
    "\n",
    "    # Get embedding for the paper\n",
    "    emb = embedder.encode(combined_text, show_progress_bar=False)\n",
    "\n",
    "    # Add to each author's sum\n",
    "    for a in authors:\n",
    "        author_id = handle_author(a)\n",
    "        if not author_id:\n",
    "            continue\n",
    "        author_sums[author_id] += emb\n",
    "        author_counts[author_id] += 1\n",
    "\n",
    "# Compute averages\n",
    "author_embeddings = {}\n",
    "for author_id, sum_vec in author_sums.items():\n",
    "    count = author_counts[author_id]\n",
    "    if count > 0:\n",
    "        author_embeddings[author_id] = sum_vec / count\n",
    "\n",
    "print(f\"✅ Computed embeddings for {len(author_embeddings)} authors\")\n",
    "\n",
    "# Convert to numpy array and save\n",
    "author_list = list(author_embeddings.keys())\n",
    "embeddings = np.vstack([author_embeddings[a] for a in author_list])\n",
    "np.save(\"author_embeddings.npy\", embeddings)\n",
    "\n",
    "# Optionally save mapping (index → author name)\n",
    "import json\n",
    "with open(\"author_index.json\", \"w\") as f:\n",
    "    json.dump(author_list, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47714f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from itertools import combinations\n",
    "from tqdm import tqdm\n",
    "G_coauthorship = nx.Graph()\n",
    "\n",
    "# --- 1. Add author nodes ---\n",
    "for author in tqdm(unique_authors):\n",
    "    G_coauthorship.add_node(author, type=\"author\", embedding=None)\n",
    "\n",
    "# --- 2. Add coauthorship edges ---\n",
    "for row in tqdm(df.itertuples(index=False), total=len(df)):\n",
    "    authors = getattr(row, 'authors_parsed', [])\n",
    "    if not isinstance(authors, list) or len(authors) < 2:\n",
    "        continue\n",
    "\n",
    "    clean_authors = [handle_author(a) for a in authors if a and isinstance(a, list)]\n",
    "\n",
    "    # Generate all coauthor pairs\n",
    "    for a1, a2 in combinations(clean_authors, 2):\n",
    "        if a1 == a2: continue\n",
    "\n",
    "        if G_coauthorship.has_edge(a1, a2):\n",
    "            # Optionally track number of coauthored papers\n",
    "            G_coauthorship[a1][a2][\"weight\"] += 1\n",
    "            G_coauthorship[a1][a2][\"papers\"].append(getattr(row, 'id', None))\n",
    "        else:\n",
    "            G_coauthorship.add_edge(a1, a2,\n",
    "                       type=\"coauthor\",\n",
    "                       weight=1,\n",
    "                       papers=[getattr(row, 'id', None)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1541f253",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check: number of nodes and edges\n",
    "print(f\"\\n✅ Coauthorship graph has {G_coauthorship.number_of_nodes()} nodes and {G_coauthorship.number_of_edges()} edges.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db509423",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "degrees = [deg for _, deg in G_coauthorship.degree()]\n",
    "print(f\"Min degree: {np.min(degrees)}\")\n",
    "print(f\"Median degree: {np.median(degrees)}\")\n",
    "print(f\"Mean degree: {np.mean(degrees):.2f}\")\n",
    "print(f\"Max degree: {np.max(degrees)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f929c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "components = sorted(nx.connected_components(G_coauthorship), key=len, reverse=True)\n",
    "print(f\"Number of components: {len(components)}\")\n",
    "print(f\"Largest component size: {len(components[0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9edca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = [d['weight'] for _,_,d in G_coauthorship.edges(data=True)]\n",
    "print(f\"Edges with >1 coauthored paper: {sum(w>1 for w in weights)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f414fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import networkx as nx\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- Load embeddings and author list ---\n",
    "embeddings = np.load(\"author_embeddings.npy\")\n",
    "with open(\"author_index.json\") as f:\n",
    "    author_list = json.load(f)\n",
    "\n",
    "# Normalize embeddings for cosine similarity\n",
    "embeddings = embeddings / np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
    "\n",
    "# --- Find nearest neighbors ---\n",
    "k = 10  # how many similar authors to connect each to\n",
    "nbrs = NearestNeighbors(n_neighbors=k+1, metric='cosine', n_jobs=-1)\n",
    "nbrs.fit(embeddings)\n",
    "distances, indices = nbrs.kneighbors(embeddings)\n",
    "\n",
    "# --- Build graph ---\n",
    "G_cosine = nx.Graph()\n",
    "\n",
    "for author in tqdm(unique_authors):\n",
    "    G_cosine.add_node(author, type=\"author\", embedding=None)\n",
    "\n",
    "for i, author in tqdm(enumerate(author_list), total=len(author_list)):\n",
    "    for j, dist in zip(indices[i][1:], distances[i][1:]):  # skip self (index 0)\n",
    "        sim = 1 - dist\n",
    "        if sim < 0.75:\n",
    "            continue\n",
    "        neighbor = author_list[j]\n",
    "        G_cosine.add_edge(author, neighbor, type=\"cosine\", weight=sim)\n",
    "\n",
    "print(f\"✅ Cosine graph has {G_cosine.number_of_nodes()} nodes and {G_cosine.number_of_edges()} edges.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139d4ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "degrees_coauthorship = [deg for _, deg in G_coauthorship.degree()]\n",
    "degrees_cosine = [deg for _, deg in G_cosine.degree()] \n",
    "\n",
    "stats_coauthorship = f\"Min: {np.min(degrees_coauthorship)}\\nMedian: {np.median(degrees_coauthorship)}\\nMean: {np.mean(degrees_coauthorship):.4f}\\nMax: {np.max(degrees_coauthorship)}\"\n",
    "stats_cosine = f\"Min: {np.min(degrees_cosine)}\\nMedian: {np.median(degrees_cosine)}\\nMean: {np.mean(degrees_cosine):.4f}\\nMax: {np.max(degrees_cosine)}\"\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.boxplot([degrees_coauthorship, degrees_cosine], labels=[f'Coauthorship_TAG\\n\\n{stats_coauthorship}', f'Cosine_Similarity_TAG\\n\\n{stats_cosine}'])\n",
    "plt.ylabel('Node Degree')\n",
    "plt.title('Degree Distribution Comparison')\n",
    "# plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb177e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get densities\n",
    "density_coauthorship = nx.density(G_coauthorship)\n",
    "density_cosine = nx.density(G_cosine)\n",
    "print(f\"Coauthorship Graph Density: {density_coauthorship:.6f}\")\n",
    "print(f\"Cosine Similarity Graph Density: {density_cosine:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b7e96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "\n",
    "# Compute degrees\n",
    "degrees_coauthorship = [deg for _, deg in G_coauthorship.degree()]\n",
    "degrees_cosine = [deg for _, deg in G_cosine.degree()]\n",
    "\n",
    "# Compute summary stats\n",
    "stats_coauthorship = (\n",
    "    f\"Nodes: {G_coauthorship.number_of_nodes()}\\n\"\n",
    "    f\"Edges: {G_coauthorship.number_of_edges()}\\n\"\n",
    "    f\"Median: {np.median(degrees_coauthorship)}\\n\"\n",
    "    f\"Mean: {np.mean(degrees_coauthorship):.1f}\\n\"\n",
    ")\n",
    "stats_cosine = (\n",
    "    f\"Nodes: {G_cosine.number_of_nodes()}\\n\"\n",
    "    f\"Edges: {G_cosine.number_of_edges()}\\n\"\n",
    "    f\"Median: {np.median(degrees_cosine)}\\n\"\n",
    "    f\"Mean: {np.mean(degrees_cosine):.1f}\\n\"\n",
    ")\n",
    "\n",
    "def degree_freq(degrees):\n",
    "    freq = Counter(degrees)\n",
    "    x, y = zip(*sorted(freq.items()))\n",
    "    return x, y\n",
    "\n",
    "x_co, y_co = degree_freq(degrees_coauthorship)\n",
    "x_cos, y_cos = degree_freq(degrees_cosine)\n",
    "\n",
    "# --- Visualization ---\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "# Boxplot comparison\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.boxplot(\n",
    "    [degrees_coauthorship, degrees_cosine],\n",
    "    labels=[f\"Coauthorship TAG\\n\\n{stats_coauthorship}\", f\"Cosine Similarity (<0.75) TAG\\n\\n{stats_cosine}\"],\n",
    "    showfliers=False,\n",
    ")\n",
    "plt.ylabel(\"Node Degree\")\n",
    "plt.title(\"Degree Distribution Comparison | Node = Author\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Histograms on log-scale to see the tail\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(x_co, y_co, 'o-', label='Coauthorship TAG', color='tab:orange', alpha=0.8)\n",
    "plt.plot(x_cos, y_cos, 'o-', label='Cosine TAG', color='tab:blue', alpha=0.8)\n",
    "plt.xlabel(f'Degree Count\\n\\nMaximum ~ Coauthor: {np.max(degrees_coauthorship)} | Cosine: {np.max(degrees_cosine)}\\nDensity ~ Coauthor: {nx.density(G_coauthorship):.6f} | Cosine: {nx.density(G_cosine):.6f}')\n",
    "plt.ylabel('Number of nodes')\n",
    "plt.title('Degree Frequency Distribution')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51dffb93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from collections import defaultdict\n",
    "\n",
    "# --- Step 1: Build mapping author -> set of paper IDs ---\n",
    "author_papers = defaultdict(set)\n",
    "for row in df.itertuples(index=False):\n",
    "    paper_id = getattr(row, 'id', None)\n",
    "    authors = getattr(row, 'authors_parsed', [])\n",
    "    if paper_id is None or not authors:\n",
    "        continue\n",
    "    for a in authors:\n",
    "        author_clean = handle_author(a)\n",
    "        if author_clean:\n",
    "            author_papers[author_clean].add(paper_id)\n",
    "\n",
    "# --- Step 2: Create a copy of the coauthorship graph ---\n",
    "G_jaccard = nx.Graph()\n",
    "G_jaccard.add_nodes_from(G_coauthorship.nodes(data=True))\n",
    "\n",
    "# --- Step 3: Compute Jaccard weight for each coauthor edge ---\n",
    "for u, v in G_coauthorship.edges():\n",
    "    papers_u = author_papers[u]\n",
    "    papers_v = author_papers[v]\n",
    "    if not papers_u or not papers_v:\n",
    "        continue\n",
    "    inter = len(papers_u & papers_v)\n",
    "    union = len(papers_u | papers_v)\n",
    "    if union == 0:\n",
    "        continue\n",
    "    w = inter / union\n",
    "    G_jaccard.add_edge(u, v, type='coauthor', weight=w)\n",
    "\n",
    "print(f\"✅ Jaccard-weighted coauthorship graph has {G_jaccard.number_of_edges()} edges.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d64c407",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print df columns names\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a1fad38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "\n",
    "# Compute degree frequency distributions\n",
    "deg_freq_coauthorship = Counter(degrees_coauthorship)\n",
    "deg_freq_cosine = Counter(degrees_cosine)\n",
    "\n",
    "# Convert to sorted arrays\n",
    "x_co, y_co = zip(*sorted(deg_freq_coauthorship.items()))\n",
    "x_cos, y_cos = zip(*sorted(deg_freq_cosine.items()))\n",
    "\n",
    "plt.figure(figsize=(7, 6))\n",
    "plt.loglog(x_co, y_co, 'o-', label='Coauthorship TAG', color='tab:blue', alpha=0.8)\n",
    "plt.loglog(x_cos, y_cos, 'o-', label='Cosine TAG', color='tab:orange', alpha=0.8)\n",
    "plt.xlabel('Degree (log scale)')\n",
    "plt.ylabel('Number of nodes (log scale)')\n",
    "plt.title('Degree Frequency Distribution (log–log)')\n",
    "plt.legend()\n",
    "plt.grid(True, which=\"both\", ls=\"--\", alpha=0.4)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de586d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(degrees_coauthorship).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d8b035",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "# Compute degree frequency distributions\n",
    "deg_freq_coauthorship = Counter(degrees_coauthorship)\n",
    "deg_freq_cosine = Counter(degrees_cosine)\n",
    "\n",
    "# Convert to sorted arrays\n",
    "x_co, y_co = zip(*sorted(deg_freq_coauthorship.items()))\n",
    "x_cos, y_cos = zip(*sorted(deg_freq_cosine.items()))\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(x_co, y_co, 'o-', label='Coauthorship TAG', color='tab:blue', alpha=0.8)\n",
    "plt.plot(x_cos, y_cos, 'o-', label='Cosine TAG', color='tab:orange', alpha=0.8)\n",
    "plt.xlabel('Degree')\n",
    "plt.ylabel('Number of nodes')\n",
    "plt.title('Degree Frequency Distribution (Linear Scale)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f75e524",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save(G, fname):\n",
    "    data = {\n",
    "        \"nodes\": list(G.nodes(data=True)), \n",
    "        \"edges\": list(G.edges(data=True))\n",
    "    }\n",
    "    with open(fname, 'w') as f: json.dump(data, f, indent=2)\n",
    "\n",
    "def load(fname):\n",
    "    G = nx.DiGraph()\n",
    "    d = json.load(open(fname))\n",
    "    G.add_nodes_from(d['nodes'])\n",
    "    G.add_edges_from(d['edges'])\n",
    "    return G\n",
    "\n",
    "def handle_author(author_parsed_instance):\n",
    "    # convert [[\"Ortega-Cerda\",\"Joaquim\",\"\"]] to Joa.Ortega-Cerda\n",
    "    first = author_parsed_instance[1]\n",
    "    last = author_parsed_instance[0]\n",
    "    author_clean = f\"{first}|{last}\"\n",
    "    return author_clean\n",
    "\n",
    "def generate_tag(df, node_type=\"article\", edge_type=\"cites\", out_dir=\"data\", limit=None):\n",
    "    if limit: df = df.head(limit)\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    G = nx.Graph(name=f\"TAG_{node_type}_{edge_type}\")\n",
    "\n",
    "    # NODE CONSTRUCTION\n",
    "    if node_type == \"article\":\n",
    "        for _, row in tqdm(df.iterrows(), total=len(df), desc=\"Adding article nodes\"):\n",
    "            node_id = row[\"id\"]\n",
    "            text = row[\"title\"] + \":\\n\" + row[\"abstract\"]\n",
    "            G.add_node(\n",
    "                node_id,\n",
    "                type=\"article\",\n",
    "                text=text,\n",
    "                embedding=embedder.encode(text).tolist(),\n",
    "                category=row[\"categories\"],\n",
    "            )\n",
    "    elif node_type == \"author\":\n",
    "        for _, row in tqdm(df.iterrows(), total=len(df), desc=\"Adding author nodes\"):\n",
    "            authors = row[\"authors_parsed\"]\n",
    "            for author in authors:\n",
    "                author_clean = handle_author(author)\n",
    "                if not G.has_node(author_clean):\n",
    "                    G.add_node(author_clean, type=\"author\", embedding=None)\n",
    "\n",
    "    # EDGE CONSTRUCTION\n",
    "\n",
    "    if edge_type == \"coauthor\":\n",
    "        if node_type != \"author\": \n",
    "            print(\"You made a mistake. Coauthor edges require author nodes. Change node_type to 'author'.\")\n",
    "            return None\n",
    "        \n",
    "        for _, row in tqdm(df.iterrows(), total=len(df), desc=\"Adding coauthor edges\"):\n",
    "            authors = row[\"authors_parsed\"]\n",
    "            for i, a1 in enumerate(authors):\n",
    "                author1_clean = handle_author(a1)\n",
    "                for a2 in authors[i + 1:]:\n",
    "                    author2_clean = handle_author(a2)\n",
    "                    G.add_edge(author1_clean, author2_clean, type=\"coauthor\", paper=row[\"id\"])\n",
    "                    \n",
    "    elif edge_type == \"cites\":\n",
    "        ids = df[\"id\"].tolist()\n",
    "        for i in range(len(ids) - 1):\n",
    "            G.add_edge(ids[i], ids[i + 1], type=\"cites\")\n",
    "\n",
    "    elif edge_type == \"co_citation\":\n",
    "        for cat, group in df.groupby(\"categories\"):\n",
    "            ids = group[\"id\"].tolist()\n",
    "            for i in range(len(ids) - 1):\n",
    "                G.add_edge(ids[i], ids[i + 1], type=\"co_citation\")\n",
    "\n",
    "    print(G)\n",
    "\n",
    "    # SAVE STUFF\n",
    "    out_path = os.path.join(out_dir, f\"TAG_{node_type}_{edge_type}.json\")\n",
    "    save(G, out_path)\n",
    "    print(f\"✅ Saved {len(G)} nodes, {G.number_of_edges()} edges → {out_path}\")\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7d5a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# METHOD [1]: COAUTHORSHIP GRAPH\n",
    "\n",
    "G_coauthorship = nx.Graph()\n",
    "article_data = {}\n",
    "\n",
    "for _, row in tqdm(df.iterrows(), total=len(df), desc=\"Building graph topology and external repo\"):\n",
    "    article_id = row[\"id\"]\n",
    "    title = row[\"title\"]\n",
    "    abstract = row[\"abstract\"]\n",
    "    article_data[article_id] = {\n",
    "        \"abstract\": title + \"\\n\" + abstract,\n",
    "        \"vector\": None\n",
    "    }\n",
    "    authors = row[\"authors_parsed\"]\n",
    "    for i, a1 in enumerate(authors):\n",
    "        author1_clean = handle_author(a1)\n",
    "        for a2 in authors[i + 1:]:\n",
    "            author2_clean = handle_author(a2)\n",
    "            # ADD NODES\n",
    "            if not G_coauthorship.has_node(author1_clean): G_coauthorship.add_node(author1_clean, type=\"author\")\n",
    "            if not G_coauthorship.has_node(author2_clean): G_coauthorship.add_node(author2_clean, type=\"author\")\n",
    "\n",
    "            # ADD/UPDATE EDGES\n",
    "            if G_coauthorship.has_edge(author1_clean, author2_clean):\n",
    "                G_coauthorship[author1_clean][author2_clean][\"paper_ids\"].append(article_id)\n",
    "                G_coauthorship[author1_clean][author2_clean][\"weight\"] += 1\n",
    "            else:\n",
    "                G_coauthorship.add_edge(author1_clean, author2_clean, paper_ids=[article_id], weight=1)\n",
    "\n",
    "for article_id, data in tqdm(article_data.items(), desc=\"Generating SBERT embeddings\"):\n",
    "    abstract = data[\"abstract\"]\n",
    "    vector = embedder.encode(abstract)\n",
    "    article_data[article_id][\"vector\"] = vector\n",
    "\n",
    "for u, v, data in tqdm(G_coauthorship.edges(data=True), desc=\"Enriching graph edges with topic embeddings\"):\n",
    "    paper_ids = data[\"paper_ids\"]\n",
    "    vectors_to_average = []\n",
    "    for pid in paper_ids:\n",
    "        vector = article_data[pid][\"vector\"]\n",
    "        vectors_to_average.append(vector)\n",
    "    mean_vector = sum(vectors_to_average) / len(vectors_to_average)\n",
    "    data[\"topic_embedding\"] = mean_vector.tolist()\n",
    "\n",
    "embeddings = []\n",
    "for article_id, data in article_data.items():\n",
    "    vector = data[\"vector\"]\n",
    "    if vector is not None:\n",
    "        embeddings.append(vector)\n",
    "\n",
    "save(G_coauthorship, \"data/TAG_author_coauthor_enriched.json\")\n",
    "embeddings_array = np.array(embeddings)\n",
    "np.save(\"data/article_embeddings.npy\", embeddings_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b409b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae86ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "G_coauthorship = load(\"data/TAG_author_coauthor_enriched.json\")\n",
    "print(\"num authors:\", G_coauthorship.number_of_nodes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ddc4160",
   "metadata": {},
   "outputs": [],
   "source": [
    "author_list = []\n",
    "for i, row in tqdm(df.iterrows(), total=len(df), desc=\"Adding nodes to topical similarity graph\"):\n",
    "    authors = row[\"authors_parsed\"]\n",
    "    for a in authors:\n",
    "        author_clean = handle_author(a)\n",
    "        if author_clean not in author_list:\n",
    "            author_list.append(author_clean)\n",
    "            G_topic_similarity.add_node(author_clean, type=\"author\", embedding=None)\n",
    "print(len(author_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49bd75f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# METHOD [2]: TOPICAL SIMILARITY GRAPH\n",
    "\n",
    "G_topic_similarity = nx.Graph()\n",
    "article_data = {}\n",
    "\n",
    "embeddings = np.load(\"data/article_embeddings.npy\")\n",
    "\n",
    "# nodes: cleaned_author, type, embeddings\n",
    "# edges: cleaned_author1, cleaned_author2, type, weight, topic_embedding\n",
    "# cleaned_author examples ~ \"P.|Bezzon\", \"R.|Menegazzo\",\n",
    "\n",
    "author_list = []\n",
    "for i, row in tqdm(df.iterrows(), total=len(df), desc=\"Adding nodes to topical similarity graph\"):\n",
    "    authors = row[\"authors_parsed\"]\n",
    "    for a in authors:\n",
    "        author_clean = handle_author(a)\n",
    "        if author_clean not in author_list:\n",
    "            author_list.append(author_clean)\n",
    "            G_topic_similarity.add_node(author_clean, type=\"author\", embedding=None)\n",
    "\n",
    "similarities = []\n",
    "# loop through embeddings to calculate cosine similarity matrix\n",
    "for embedding in embeddings:\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for i, u in tqdm(enumerate(G_topic_similarity.nodes()), total=G_topic_similarity.number_of_nodes(), desc=\"Calculating edge similarity matrix\"):\n",
    "    # vec_u = article_data[u][\"vector\"]\n",
    "    # for j, v in enumerate(G_topic_similarity.nodes()):\n",
    "    #     if i >= j: continue\n",
    "    #     vec_v = article_data[v][\"vector\"]\n",
    "    #     similarity = np.dot(vec_u, vec_v) / (np.linalg.norm(vec_u) * np.linalg.norm(vec_v))\n",
    "    #     similarities.append(similarity)\n",
    "    #     if similarity >= 0.25:\n",
    "    #         G_topic_similarity.add_edge(u, v, type=\"topic_similarity\", weight=similarity)\n",
    "    \n",
    "    # ADD EDGES BETWEEN AUTHORS BASED ON TOPICAL SIMILARITY\n",
    "    for \n",
    "\n",
    "save(G_topic_similarity, \"data/TAG_author_topic_similarity.json\")\n",
    "print(pd.Series(similarities).describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5e2cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save(G_topic_similarity, \"data/TAG_article_topic_similarity.json\")\n",
    "np.save(\"data/t.npy\", embeddings_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c741badc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(similarities).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f57c5da",
   "metadata": {},
   "outputs": [],
   "source": [
    "len([s for s in similarities if s >= 0.25]) / len(similarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377c5108",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print np stats for similarities\n",
    "print(\"Similarity stats:\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248b858a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.columns)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56bbd1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(set(df['categories'])))\n",
    "print(set(df['categories']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8dfb638",
   "metadata": {},
   "outputs": [],
   "source": [
    "category_broad = set()\n",
    "for category in set(df['categories']):\n",
    "    category.split('.')\n",
    "    print(category, '|', category.split('.')[0])\n",
    "    category_broad.add(category.split('.')[0])\n",
    "\n",
    "print(len(category_broad))\n",
    "print(category_broad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df82584e",
   "metadata": {},
   "outputs": [],
   "source": [
    "'cs' in category_broad\n",
    "# where is cs in category_broad\n",
    "print([category for category in category_broad if category == 'cs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206ed3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add category_broad to df as integer labels\n",
    "category_to_int = {cat: idx for idx, cat in enumerate(sorted(category_broad))}\n",
    "df['category_broad'] = df['categories'].apply(lambda cat: category_to_int[cat.split('.')[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f2b8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save g_coauthorship and g_cosine\n",
    "save(G_coauthorship, \"data/TAG_author_coauthorship.json\")\n",
    "save(G_cosine, \"data/TAG_author_cosine.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d6b5b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === GNN training cell: PyTorch Geometric ===\n",
    "import random\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import normalize\n",
    "from torch_geometric.utils import from_networkx, to_undirected\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import SAGEConv\n",
    "import networkx as nx\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --------------------\n",
    "# CONFIG\n",
    "# --------------------\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "HIDDEN_DIM = 256\n",
    "NUM_LAYERS = 2\n",
    "DROPOUT = 0.5\n",
    "LR = 1e-3\n",
    "WEIGHT_DECAY = 1e-5\n",
    "EPOCHS = 200\n",
    "RANDOM_SEED = 42\n",
    "NUM_CLASSES = 174   # your number of category_broad classes\n",
    "USE_DEG_FEATURE = False  # optionally append degree as a scalar feature\n",
    "\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "random.seed(RANDOM_SEED)\n",
    "\n",
    "# --------------------\n",
    "# Utilities: build author -> paper set and target distribution\n",
    "# --------------------\n",
    "# Build author -> paper id map (if not already available)\n",
    "author_papers = defaultdict(set)\n",
    "for row in df.itertuples(index=False):\n",
    "    pid = getattr(row, 'id', None)\n",
    "    authors = getattr(row, 'authors_parsed', [])\n",
    "    if pid is None: \n",
    "        continue\n",
    "    for a in authors:\n",
    "        auth = handle_author(a)\n",
    "        if auth:\n",
    "            author_papers[auth].add(pid)\n",
    "\n",
    "# Build author -> category distribution (y)\n",
    "# Assumption: df has 'category_broad' with integer label in [0, NUM_CLASSES-1]\n",
    "# We'll compute for each author a normalized histogram over categories of their papers.\n",
    "author_category_counts = defaultdict(lambda: np.zeros(NUM_CLASSES, dtype=np.float32))\n",
    "for row in df.itertuples(index=False):\n",
    "    pid = getattr(row, 'id', None)\n",
    "    cat = getattr(row, 'category_broad', None)\n",
    "    if pid is None or cat is None: \n",
    "        continue\n",
    "    authors = getattr(row, 'authors_parsed', [])\n",
    "    for a in authors:\n",
    "        auth = handle_author(a)\n",
    "        if auth:\n",
    "            author_category_counts[auth][int(cat)] += 1.0\n",
    "\n",
    "# Normalize to probability distributions (sum to 1). If an author had zero, we'll leave zeros (mask later).\n",
    "author_list_all = author_list  # order used for embeddings\n",
    "author_to_idx = {a: i for i, a in enumerate(author_list_all)}\n",
    "N = len(author_list_all)\n",
    "\n",
    "Y = np.zeros((N, NUM_CLASSES), dtype=np.float32)\n",
    "for a, idx in author_to_idx.items():\n",
    "    counts = author_category_counts.get(a, None)\n",
    "    if counts is None:\n",
    "        continue\n",
    "    s = counts.sum()\n",
    "    if s > 0:\n",
    "        Y[idx] = counts / s\n",
    "    else:\n",
    "        Y[idx] = np.zeros(NUM_CLASSES, dtype=np.float32)\n",
    "\n",
    "# --------------------\n",
    "# Node features: embeddings (+ optional degree)\n",
    "# --------------------\n",
    "# embeddings: numpy array shape (N, 384)\n",
    "# Normalize embedding vectors\n",
    "embeddings = normalize(embeddings, axis=1)  # L2 normalize rows\n",
    "X = embeddings.astype(np.float32)\n",
    "\n",
    "if USE_DEG_FEATURE:\n",
    "    # compute degree for nodes using G_coauthorship as baseline (or G_cosine)\n",
    "    deg_map = {}\n",
    "    for g in [G_coauthorship, G_cosine]:\n",
    "        if len(g) > 0:\n",
    "            # default deg from coauthorship for feature; you can choose per graph later\n",
    "            break\n",
    "    # use degrees from G_coauthorship\n",
    "    degs = np.zeros((N, 1), dtype=np.float32)\n",
    "    for a, i in author_to_idx.items():\n",
    "        d = G_coauthorship.degree(a) if a in G_coauthorship else 0\n",
    "        degs[i, 0] = float(d)\n",
    "    # normalize degree (log scaling helps)\n",
    "    degs = np.log1p(degs)\n",
    "    degs = (degs - degs.mean()) / (degs.std() + 1e-9)\n",
    "    X = np.concatenate([X, degs], axis=1)\n",
    "\n",
    "FEATURE_DIM = X.shape[1]\n",
    "\n",
    "# --------------------\n",
    "# Helper: convert networkx graph -> torch_geometric Data\n",
    "# --------------------\n",
    "def build_data_from_nx(G_nx, x_array, y_array, train_frac=0.7, val_frac=0.15, test_frac=0.15):\n",
    "    # Ensure graph contains all author nodes\n",
    "    # If some author nodes are missing from G_nx, we'll add isolated nodes\n",
    "    G_copy = G_nx.copy()\n",
    "    for a in author_list_all:\n",
    "        if a not in G_copy:\n",
    "            G_copy.add_node(a)\n",
    "    # convert to PyG Data\n",
    "\n",
    "\n",
    "    pyg = from_networkx(G_copy)  # node attribute ordering depends on networkx; we'll set x, y manually\n",
    "    # Build edge_index (undirected)\n",
    "\n",
    "    edges = [(author_to_idx[u], author_to_idx[v]) for u, v in G_copy.edges()]\n",
    "    if len(edges) == 0:\n",
    "        edge_index = torch.empty((2, 0), dtype=torch.long)\n",
    "    else:\n",
    "        edge_index = torch.tensor(np.array(edges).T, dtype=torch.long)\n",
    "    edge_index = to_undirected(edge_index)\n",
    "\n",
    "    # edge_index = torch.tensor(np.array(list(G_copy.edges())).T, dtype=torch.long)\n",
    "    # if edge_index.numel() == 0:\n",
    "    #     edge_index = torch.empty((2, 0), dtype=torch.long)\n",
    "    # else:\n",
    "    #     edge_index = to_undirected(edge_index)\n",
    "\n",
    "\n",
    "    # Build data object\n",
    "    data = Data()\n",
    "    data.num_nodes = N\n",
    "    data.edge_index = edge_index.to(DEVICE)\n",
    "    data.x = torch.tensor(x_array, dtype=torch.float32).to(DEVICE)\n",
    "    data.y = torch.tensor(y_array, dtype=torch.float32).to(DEVICE)\n",
    "    # Build masks (random split on nodes that have non-zero target)\n",
    "    has_target = (data.y.sum(dim=1) > 0).cpu().numpy()\n",
    "    idx_with_target = np.where(has_target)[0]\n",
    "    train_idx, test_idx = train_test_split(idx_with_target, train_size=train_frac, random_state=RANDOM_SEED)\n",
    "    val_size = int(len(train_idx) * (val_frac / train_frac))\n",
    "    # split train_idx into train/val\n",
    "    train_idx, val_idx = train_test_split(train_idx, test_size=val_size, random_state=RANDOM_SEED)\n",
    "    mask_train = np.zeros(N, dtype=bool)\n",
    "    mask_val = np.zeros(N, dtype=bool)\n",
    "    mask_test = np.zeros(N, dtype=bool)\n",
    "    mask_train[train_idx] = True\n",
    "    mask_val[val_idx] = True\n",
    "    mask_test[test_idx] = True\n",
    "    data.train_mask = torch.tensor(mask_train, dtype=torch.bool).to(DEVICE)\n",
    "    data.val_mask = torch.tensor(mask_val, dtype=torch.bool).to(DEVICE)\n",
    "    data.test_mask = torch.tensor(mask_test, dtype=torch.bool).to(DEVICE)\n",
    "    return data\n",
    "\n",
    "# Build Data objects for both graphs\n",
    "data_co = build_data_from_nx(G_coauthorship, X, Y)\n",
    "data_cos = build_data_from_nx(G_cosine, X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a244948b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------\n",
    "# Model: 2-layer GraphSAGE + MLP head\n",
    "# --------------------\n",
    "class SAGEPredictor(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim, out_dim, dropout=0.5):\n",
    "        super().__init__()\n",
    "        self.conv1 = SAGEConv(in_dim, hidden_dim)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_dim)\n",
    "        self.conv2 = SAGEConv(hidden_dim, hidden_dim)\n",
    "        self.bn2 = nn.BatchNorm1d(hidden_dim)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim//2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim//2, out_dim)\n",
    "        )\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        out = self.mlp(x)  # logits for classes\n",
    "        log_probs = F.log_softmax(out, dim=-1)  # log-probs for KLDivLoss\n",
    "        return log_probs\n",
    "\n",
    "# --------------------\n",
    "# Training & eval helpers\n",
    "# --------------------\n",
    "from torch import optim\n",
    "from torch.nn import KLDivLoss\n",
    "from sklearn.metrics import top_k_accuracy_score\n",
    "import math\n",
    "\n",
    "def train_one(model, data, optimizer, criterion):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data.x, data.edge_index)\n",
    "    # compute loss only on training nodes with targets\n",
    "    mask = data.train_mask\n",
    "    target = data.y[mask]\n",
    "    pred = out[mask]\n",
    "    loss = criterion(pred, target)  # KLDiv between log_prob and target prob\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, data, mask):\n",
    "    model.eval()\n",
    "    out = model(data.x, data.edge_index)  # log-probs\n",
    "    probs = out.exp().cpu().numpy()\n",
    "    targets = data.y.cpu().numpy()\n",
    "    mask_np = mask.cpu().numpy()\n",
    "    if mask_np.sum() == 0:\n",
    "        return {\"kl\": float('nan'), \"top1\": float('nan'), \"top3\": float('nan'), \"cosine\": float('nan')}\n",
    "    pred = probs[mask_np]\n",
    "    targ = targets[mask_np]\n",
    "    # KL (batch mean)\n",
    "    # reconvert pred to log for criterion\n",
    "    kl = float(np.sum(targ * (np.log(np.maximum(targ, 1e-12)) - np.log(np.maximum(pred, 1e-12)))) / len(pred))\n",
    "    # top-1 / top-3 accuracy (if targets are distributions, use argmax)\n",
    "    y_true_idx = targ.argmax(axis=1)\n",
    "    y_pred_idx = pred.argmax(axis=1)\n",
    "    top1 = (y_pred_idx == y_true_idx).mean()\n",
    "    # top-3\n",
    "    # top3 = top_k_accuracy_score(y_true_idx, pred, k=3)\n",
    "    top3 = top_k_accuracy_score(y_true_idx, pred, k=3, labels=np.arange(NUM_CLASSES))\n",
    "    # cosine similarity between pred and true vectors (mean)\n",
    "    dot = (pred * targ).sum(axis=1)\n",
    "    norm_pred = np.linalg.norm(pred, axis=1)\n",
    "    norm_targ = np.linalg.norm(targ, axis=1)\n",
    "    cosines = dot / (norm_pred * norm_targ + 1e-12)\n",
    "    cosine_mean = float(np.nanmean(cosines))\n",
    "    return {\"kl\": kl, \"top1\": top1, \"top3\": top3, \"cosine\": cosine_mean}\n",
    "\n",
    "# --------------------\n",
    "# Training loop function: train on a given data object (coauthorship or cosine)\n",
    "# --------------------\n",
    "def run_training(data, name=\"graph\"):\n",
    "    model = SAGEPredictor(FEATURE_DIM, HIDDEN_DIM, NUM_CLASSES, dropout=DROPOUT).to(DEVICE)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "    criterion = KLDivLoss(reduction='batchmean')  # expects log_prob inputs and target probs\n",
    "    best_val_kl = float('inf')\n",
    "    best_state = None\n",
    "\n",
    "    for epoch in range(1, EPOCHS + 1):\n",
    "        loss = train_one(model, data, optimizer, criterion)\n",
    "        train_metrics = evaluate(model, data, data.train_mask)\n",
    "        val_metrics = evaluate(model, data, data.val_mask)\n",
    "        if val_metrics[\"kl\"] < best_val_kl:\n",
    "            best_val_kl = val_metrics[\"kl\"]\n",
    "            best_state = model.state_dict()\n",
    "        if epoch % 10 == 0 or epoch == 1:\n",
    "            print(f\"[{name}] Epoch {epoch:03d} | Loss {loss:.4f} | Train KL {train_metrics['kl']:.4f} | Val KL {val_metrics['kl']:.4f} | Val top1 {val_metrics['top1']:.3f} | Val cos {val_metrics['cosine']:.3f}\")\n",
    "\n",
    "    # load best\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "    test_metrics = evaluate(model, data, data.test_mask)\n",
    "    print(f\"*** [{name}] TEST KL {test_metrics['kl']:.4f} | top1 {test_metrics['top1']:.3f} | top3 {test_metrics['top3']:.3f} | cos {test_metrics['cosine']:.3f}\")\n",
    "    return model, test_metrics\n",
    "\n",
    "# --------------------\n",
    "# Run experiments on both TAGs\n",
    "# --------------------\n",
    "print(\"Training on Coauthorship TAG...\")\n",
    "model_co, metrics_co = run_training(data_co, name=\"Coauthorship\")\n",
    "\n",
    "print(\"\\nTraining on Cosine TAG...\")\n",
    "model_cos, metrics_cos = run_training(data_cos, name=\"Cosine\")\n",
    "\n",
    "# Optionally save models\n",
    "torch.save(model_co.state_dict(), \"gnn_coauthorship.pt\")\n",
    "torch.save(model_cos.state_dict(), \"gnn_cosine.pt\")\n",
    "print(\"Models saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6805752c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_columns = ['gnn_only', 'llm_only', 'sequential', 'dual']\n",
    "# sub_accuracies for coauthorship, cosine, topic_similarity\n",
    "sub_accuracies = [[0.891, 0.891, 0], [0,0,0], [0,0,0], [0,0,0]]\n",
    "\n",
    "# plot grouped bar chart for sub_accuracies\n",
    "x = np.arange(len(sub_columns))\n",
    "width = 0.2\n",
    "fig, ax = plt.subplots(figsize=(10,6))\n",
    "bars1 = ax.bar(x - width, [sub_accuracies[i][0] for i in range(len(sub_columns))], width, label='Coauthorship TAG', color='tab:red')\n",
    "bars2 = ax.bar(x, [sub_accuracies[i][1] for i in range(len(sub_columns))], width, label='Cosine TAG', color='tab:blue')\n",
    "bars3 = ax.bar(x + width, [sub_accuracies[i][2] for i in range(len(sub_columns))], width, label='Topic_Similarity TAG', color='tab:green')\n",
    "ax.set_ylabel('Accuracy')\n",
    "ax.set_title('GNN Subtask Accuracies by TAG Type')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(sub_columns)\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c815c926",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ---------------------------\n",
    "# Setup placeholder data\n",
    "# ---------------------------\n",
    "graph_types = [\"Coauthorship\", \"Cosine\", \"Topic Similarity\"]\n",
    "models = [\"GNN Only\", \"LLM Only\", \"Sequential\", \"Dual\"]\n",
    "metrics = [\"KL ↓\", \"Top-1 ↑\", \"Top-3 ↑\", \"Cosine ↑\"]\n",
    "\n",
    "results = {\n",
    "    \"Coauthorship\": {\n",
    "        \"GNN Only\":  [0.4555, 0.891, 0.946, 0.908],\n",
    "        \"LLM Only\":  [0, 0, 0, 0],\n",
    "        \"Sequential\": [0, 0, 0, 0],\n",
    "        \"Dual\":      [0, 0, 0, 0],\n",
    "    },\n",
    "    \"Cosine\": {\n",
    "        \"GNN Only\":  [0.4844, 0.891, 0.942, 0.905],\n",
    "        \"LLM Only\":  [0, 0, 0, 0],\n",
    "        \"Sequential\": [0, 0, 0, 0],\n",
    "        \"Dual\":      [0, 0, 0, 0],\n",
    "    },\n",
    "    \"Topic Similarity\": {\n",
    "        \"GNN Only\":  [0, 0, 0, 0],\n",
    "        \"LLM Only\":  [0, 0, 0, 0],\n",
    "        \"Sequential\": [0, 0, 0, 0],\n",
    "        \"Dual\":      [0, 0, 0, 0],\n",
    "    },\n",
    "}\n",
    "\n",
    "# ---------------------------\n",
    "# Build table data with separator rows\n",
    "# ---------------------------\n",
    "table_data = []\n",
    "for i, graph in enumerate(graph_types):\n",
    "    for model in models:\n",
    "        row = [graph, model] + [f\"{v:.3f}\" if v != 0 else \"–\" for v in results[graph][model]]\n",
    "        table_data.append(row)\n",
    "    # Add a black separator row except after the last graph\n",
    "    if i < len(graph_types) - 1:\n",
    "        table_data.append([\"\", \"\", \"\", \"\", \"\", \"\"])\n",
    "\n",
    "columns = [\"Graph\", \"Model\"] + metrics\n",
    "\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Plot table\n",
    "# ---------------------------\n",
    "table_data_with_spacing = []\n",
    "for i, graph in enumerate(graph_types):\n",
    "    for model in models:\n",
    "        row = [graph, model] + [f\"{v:.3f}\" if v != 0 else \"–\" for v in results[graph][model]]\n",
    "        table_data_with_spacing.append(row)\n",
    "    # Add a blank row for spacing except after the last graph\n",
    "    if i < len(graph_types) - 1:\n",
    "        table_data_with_spacing.append([\"\"] * (2 + len(metrics)))  # empty row\n",
    "\n",
    "# Then when plotting the table, just increase row height slightly\n",
    "fig, ax = plt.subplots(figsize=(9, 5))\n",
    "ax.axis(\"off\")\n",
    "tbl = ax.table(\n",
    "    cellText=table_data_with_spacing,\n",
    "    colLabels=columns,\n",
    "    loc=\"center\",\n",
    "    cellLoc=\"center\",\n",
    ")\n",
    "tbl.auto_set_font_size(False)\n",
    "tbl.set_fontsize(10)\n",
    "tbl.scale(1.2, 1.6)  # slightly taller rows for visual spacing\n",
    "\n",
    "\n",
    "# Style headers\n",
    "for (row, col), cell in tbl.get_celld().items():\n",
    "    if row == 0:\n",
    "        cell.set_text_props(weight=\"bold\", color=\"white\")\n",
    "        cell.set_facecolor(\"#4C72B0\")\n",
    "    elif row % 5 == 0 and row not in [0, len(table_data)]:  \n",
    "        # gray background for alternating graph sections\n",
    "        cell.set_facecolor(\"#f0f0f0\")\n",
    "    elif all(c == \"\" for c in table_data[row-1]):\n",
    "\n",
    "        # Make the spacer rows black\n",
    "        cell.set_facecolor(\"green\")\n",
    "        cell.get_text().set_color(\"white\")  # optional, to make text visible if any\n",
    "\n",
    "\n",
    "plt.title(\"Performance Summary Across Graph Variants and Models\", pad=20)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a39dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import top_k_accuracy_score\n",
    "\n",
    "# Assume NUM_CLASSES = 174\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
